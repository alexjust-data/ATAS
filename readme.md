
# ðŸ§  RIESGO_PERDIDA â€” ATAS FIFO Trade Loader

Reconstruye trades desde archivos Excel generados por ATAS, fusiona fragmentos, calcula estadÃ­sticas y valida la integridad de los datos. Soporta capital inicial dinÃ¡mico y conexiÃ³n a PostgreSQL.

## ðŸ“¦ Estructura del proyecto

```bash

RIESGO\_PERDIDA/
â”œâ”€â”€ core/                  # LÃ³gica modular del sistema
â”‚   â”œâ”€â”€ **init**.py
â”‚   â”œâ”€â”€ capital.py         # Capital inicial (env, prompt, global)
â”‚   â”œâ”€â”€ config.py          # Paths, logging, variables globales
â”‚   â”œâ”€â”€ db\_utils.py        # Reset y test de base de datos PostgreSQL
â”‚   â”œâ”€â”€ fifo\_loader.py     # ReconstrucciÃ³n de trades FIFO desde Excel
â”‚   â”œâ”€â”€ io\_utils.py        # Carga/guardado de CSV, SQL, Excel
â”‚   â”œâ”€â”€ merger.py          # Unifica trades fragmentados + explicaciÃ³n
â”‚   â”œâ”€â”€ pipeline.py        # Orquesta el flujo completo de ingesta
â”‚   â”œâ”€â”€ summary.py         # KPIs diarios, winrate, profit factor
â”‚   â””â”€â”€ qa.py              # Validaciones QA sobre el dataframe
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 00\_overview\.ipynb  # GuÃ­a general del proyecto
â”‚   â””â”€â”€ ejemplo.ipynb
â”œâ”€â”€ input/                 # Archivos Excel fuente (.xlsx)
â”œâ”€â”€ output/                # Datos ya procesados (.csv)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

````

---

## âš™ï¸ InstalaciÃ³n

```bash
git clone https://github.com/tuusuario/RIESGO_PERDIDA.git
cd RIESGO_PERDIDA
python -m venv trading_env
source trading_env/bin/activate
pip install -r requirements.txt
````

Configura tu `.env` con tu `DATABASE_URL` y capital inicial si lo deseas:

```dotenv
DATABASE_URL=postgresql+psycopg2://alex@localhost:5432/trading
INITIAL_CAPITAL=10000
```

---

## ðŸš€ Uso rÃ¡pido en Jupyter

Agrega esto al inicio del notebook:

```python
# notebooks/00_overview.ipynb
import sys
from pathlib import Path

project_root = Path("..").resolve()
if str(project_root) not in sys.path:
    sys.path.append(str(project_root))

from core import process_new_files, daily_summary_from_hist
df = process_new_files()
df_summary = daily_summary_from_hist(df)
df_summary.tail()
```

---

## ðŸ” Chequeos QA incluidos

Desde `core.qa` puedes hacer:

```python
from core.qa import check_integrity, debug_components

check_integrity(df)  # Lanza errores si encuentra inconsistencias
debug_components(df) # DiagnÃ³stico de trades fragmentados incorrectos
```

### Tests automÃ¡ticos implementados

| Test                        | QuÃ© comprueba                                                   |
| --------------------------- | --------------------------------------------------------------- |
| **IDs Ãºnicos**              | No existan `trade_id` duplicados                                |
| **Orden-IDs coherentes**    | Que `order_id_entry` y `order_id_exit` no estÃ©n duplicados      |
| **VolÃºmenes positivos**     | Que `position_size > 0`                                         |
| **Signo de PnL**            | Que el signo de `PnL` coincida con `PnL_net`                    |
| **Equity creciente**        | Que `equity = capital + PnL_net.cumsum()`                       |
| **Fragmentos consistentes** | Que los `components` coincidan con el `position_size` declarado |

---

## ðŸ“˜ DocumentaciÃ³n clave

* [`notebooks/00_overview.ipynb`](notebooks/00_overview.ipynb): guÃ­a completa del flujo de uso.
* [`core/qa.py`](core/qa.py): incluye todos los tests y validaciones.

---

## ðŸ§ª Reproducibilidad

Usa `process_new_files()` para ingerir todos los Excel de `input/`, guardarlos en PostgreSQL y generar un CSV acumulado.

```python
df = process_new_files()
```

