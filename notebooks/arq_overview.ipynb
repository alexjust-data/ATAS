{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "833d8faa",
   "metadata": {},
   "source": [
    "### 00 – Architecture Overview (RAW NOTEBOOK)\n",
    "\n",
    "**1. Data sources**\n",
    "| Source | Format | Frequency | Owner |\n",
    "|--------|--------|-----------|-------|\n",
    "| ATAS trade exports | `.xlsx` | Manual (user upload) | Trading desk |\n",
    "| Screen captures | `.png` | Real‑time (stream) | Pattern‑recog service |\n",
    "| Economic indicators | REST/CSV | ‑15 min lag | External APIs |\n",
    "| News feeds | RSS/JSON | Streaming | News API |\n",
    "| Options chain | REST/CSV | EOD + intraday | Broker API |\n",
    "\n",
    "---\n",
    "**2. Ingestion patterns**\n",
    "* **Push** – user drops files into `input/` → landing bucket.\n",
    "* **Push** – screen‑capture microservice pushes JPEG/PNG to stream.\n",
    "* **Pull** – cron jobs query APIs for macro, options, news.\n",
    "\n",
    "We tag each record with `source`, `ingest_ts`, `version`.\n",
    "\n",
    "---\n",
    "**3. Storage layers**\n",
    "```\n",
    "┌────────────┐      ┌────────────┐      ┌────────────┐      ┌────────────┐\n",
    "│  Landing   │ →    │  Staging   │ →    │  Feature   │ →    │   ML Ops   │\n",
    "│  (S3 raw)  │      │ (Parquet)  │      │  Store     │      │ Artifacts  │\n",
    "└────────────┘      └────────────┘      └────────────┘      └────────────┘\n",
    "```\n",
    "* **Landing** – append‑only, immutable, same format as received.\n",
    "* **Staging** – columnar Parquet, partitioned (date/source).\n",
    "* **Feature Store** – ready for ML (DB, e.g. DuckDB/Redshift).\n",
    "* **Artifacts** – model weights, metrics, notebooks (S3).\n",
    "\n",
    "---\n",
    "**4. Processing workflows**\n",
    "| Step | Tool | Notes |\n",
    "|------|------|-------|\n",
    "| Validation  | `pandas‑schema` | shape, dtypes, nulls |\n",
    "| Transformation | `pandas` / `polars` | cast, enrich |\n",
    "| Image vectorisation | `torch` + CNN | converts PNG → embedding |\n",
    "| Join & dedup | SQL | stage → feature store |\n",
    "\n",
    "Trigger types:\n",
    "* Batch (cron, hourly) for files & APIs.\n",
    "* Stream (on message) for images.\n",
    "\n",
    "---\n",
    "**5. Real‑time pipeline**\n",
    "```\n",
    "Screen → WebSocket → Queue → CV service → Feature Store → Alert engine\n",
    "```\n",
    "Latency target < **2 s**.\n",
    "Queue = Kafka/Kinesis. CV service publishes detected pattern (`signal`, `prob`, `ts`). Alert engine pushes to GUI / mobile.\n",
    "\n",
    "---\n",
    "**6. Batch pipeline (trades, options, macro)**\n",
    "1. **Ingest** landing files.\n",
    "2. **ETL** (Glue/Airflow) → staging.\n",
    "3. **Aggregate** daily P&L, greeks.\n",
    "4. **Persist** to analytics DB.\n",
    "5. **Train** nightly models.\n",
    "\n",
    "---\n",
    "**7. Orchestration & lineage**\n",
    "* Batch – Apache Airflow DAGs.\n",
    "* Stream – Managed Kinesis + Lambda.\n",
    "* Metadata – OpenLineage tags (dataset, job, run_id).\n",
    "\n",
    "---\n",
    "**8. Schemas (staging)**\n",
    "```sql\n",
    "CREATE TABLE trades_raw(\n",
    "    trade_id BIGINT,\n",
    "    symbol TEXT,\n",
    "    timestamp TIMESTAMPTZ,\n",
    "    qty NUMERIC,\n",
    "    price NUMERIC,\n",
    "    source TEXT,\n",
    "    ingest_ts TIMESTAMPTZ,\n",
    "    version INT\n",
    ");\n",
    "\n",
    "CREATE TABLE news_raw(\n",
    "    uuid TEXT,\n",
    "    headline TEXT,\n",
    "    body TEXT,\n",
    "    published TIMESTAMPTZ,\n",
    "    sentiment NUMERIC,\n",
    "    ingest_ts TIMESTAMPTZ\n",
    ");\n",
    "```\n",
    "(Analogous tables for `options_raw`, `macro_raw`, `img_signals`.)\n",
    "\n",
    "---\n",
    "**9. Model training & online inference**\n",
    "| Type | Algo | Freshness |\n",
    "|------|------|-----------|\n",
    "| Pattern recog | CNN/ViT | realtime |\n",
    "| News sentiment | fin‑BERT | ≤5 min |\n",
    "| Signal ensemble | GradientBoost | hourly |\n",
    "\n",
    "Models log to MLflow; on update, inference endpoint hot‑swaps.\n",
    "\n",
    "---\n",
    "**10. Alerting & monitoring**\n",
    "* Data quality – Great Expectations, alerts via Slack.\n",
    "* Pipeline – AWS CloudWatch / Grafana.\n",
    "* Model drift – statistical tests + alert.\n",
    "\n",
    "---\n",
    "**11. Security & access**\n",
    "* S3 bucket policies: landing is write‑only, staging read‑only for analysts.\n",
    "* IAM roles per microservice.\n",
    "* Secrets in AWS Secrets Manager.\n",
    "\n",
    "---\n",
    "**12. Next tasks**\n",
    "1. Finalise schema for `img_signals`.\n",
    "2. Prototype Kinesis producer for screen‑capture.\n",
    "3. Define FE pipeline (stats feats) in Feature Store.\n",
    "4. Draft Airflow DAG for trades ETL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdc73ca",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
